{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- is_valid == False  : Run cells between \" Test Data Inference \" and \" Entity level  Performance ( Evaluate ) \"\n",
    "- is_valid == True   : Run cells below \" Entity level  Performance ( Evaluate ) \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "from net import BiLSTMCRF, BERTCRF, BERTBiLSTMCRF, BERTransformerCRF\n",
    "from forward_step import *\n",
    "import pickle\n",
    "import collections\n",
    "from json import loads, dumps\n",
    "from collections import OrderedDict\n",
    "\n",
    "gpu = 1\n",
    "device = torch.device('cuda:%d'%(gpu) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "####################################################################\n",
    "###################### Load data & preprocess ######################\n",
    "####################################################################\n",
    "# 'bert-base-chinese' 'hfl/chinese-roberta-wwm-ext'\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "with open('./token_info/ner2int.pickle', 'rb') as file:\n",
    "    ner2int = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "def str_split(x):\n",
    "    return x.split(' ')\n",
    "\n",
    "def ner_onehot(x):\n",
    "    return [ner2int[k] for k in x]\n",
    "\n",
    "CONTENT = data.Field(sequential = True, tokenize=str_split, preprocessing=tokenizer.convert_tokens_to_ids, pad_token=tokenizer.pad_token_id, unk_token=tokenizer.unk_token_id, fix_length=50, use_vocab = False, lower = True, batch_first = True, include_lengths = False)\n",
    "NER = data.Field(sequential = True, tokenize=str_split, preprocessing=ner_onehot, pad_token=len(ner2int), fix_length=50, use_vocab = False, lower = False, batch_first = True, include_lengths = False)\n",
    "ID = data.LabelField(sequential = False, tokenize = str, dtype=torch.float, lower = False, use_vocab = False)\n",
    "\n",
    "\n",
    "if is_valid:\n",
    "    DataSet = data.TabularDataset(\n",
    "        path='./data/cluener_dev_v2.csv', format='csv',\n",
    "        fields={'id': ('id', ID),\n",
    "                'content': ('content', CONTENT),\n",
    "                'ner': ('ner', NER)}\n",
    "        )\n",
    "else:\n",
    "    DataSet = data.TabularDataset(\n",
    "        path='./data/cluener_test_v2.csv', format='csv',\n",
    "        fields={'id': ('id', ID),\n",
    "                'content': ('content', CONTENT)}\n",
    "        )\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "valid_iterator = data.BucketIterator(\n",
    "    DataSet,\n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2ner = {}\n",
    "for i in ner2int:\n",
    "    int2ner[ner2int[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_padding_mask_bert(x, device):\n",
    "    atten_mask = torch.zeros(x.shape, dtype=torch.uint8).to(device)\n",
    "    atten_mask = atten_mask.masked_fill(x != tokenizer.pad_token_id, 1)\n",
    "    return atten_mask\n",
    "\n",
    "def inference(batch, model, device, inference=False):\n",
    "    model.eval()\n",
    "    crf_mask = key_padding_mask_bert(batch.content, device)\n",
    "    if not inference:\n",
    "        logits = model(batch, crf_mask)\n",
    "        return logits\n",
    "    else:\n",
    "        pred = model(batch, crf_mask, True)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_extract(x):\n",
    "    res = {}\n",
    "    x = [int2ner[y] for y in x]\n",
    "    index = []\n",
    "    for i, k in enumerate(x):\n",
    "        if k != 'O':\n",
    "            if not index:\n",
    "                if k[0] == 'B':\n",
    "                    if k[2:] not in res:  \n",
    "                        res[k[2:]] = []\n",
    "                    entity = k[2:]\n",
    "                    index.append(i)\n",
    "            else:\n",
    "                if k[0] == 'I':\n",
    "                    if k[2:] == entity:\n",
    "                        index.append(i)\n",
    "                    else:\n",
    "                        res[entity].append([index[0], index[-1]])\n",
    "                        index = []\n",
    "                if k[0] == 'B':\n",
    "                    res[entity].append([index[0], index[-1]])\n",
    "                    index = []\n",
    "                    if k[2:] not in res:  \n",
    "                        res[k[2:]] = []\n",
    "                    entity = k[2:]\n",
    "                    index.append(i)     \n",
    "        else:\n",
    "            if index:\n",
    "                res[entity].append([index[0], index[-1]])\n",
    "                index = []\n",
    "    if index:\n",
    "        res[entity].append([index[0], index[-1]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_entity(TP, FP, FN):\n",
    "    df = pd.DataFrame(columns=['entity', 'recall', 'precision', 'f1-score'])\n",
    "    TP_overall, FP_overall, FN_overall = 0, 0, 0\n",
    "    for i, e in enumerate(['name', 'organization', 'position', 'company', 'address', \n",
    "'game', 'government', 'scene', 'book', 'movie']):\n",
    "        TP_overall += TP[e]\n",
    "        FP_overall += FP[e]\n",
    "        FN_overall += FN[e]\n",
    "        recall_entity = TP[e]/(TP[e]+FN[e])\n",
    "        precision_entity = TP[e]/(TP[e]+FP[e])\n",
    "        f1_score_entity = 2*(recall_entity*precision_entity)/(recall_entity+precision_entity)\n",
    "        df.loc[i] = [e, recall_entity, precision_entity, f1_score_entity]\n",
    "    recall_overall = TP_overall/(TP_overall+FN_overall)\n",
    "    precision_overall = TP_overall/(TP_overall+FP_overall)\n",
    "    f1_score_overall = 2*(recall_overall*precision_overall)/(recall_overall+precision_overall)\n",
    "    recall_ma = df['recall'].mean()\n",
    "    precision_ma = df['precision'].mean()\n",
    "    f1_score_ma = 2*(recall_ma*precision_ma)/(recall_ma+precision_ma)  \n",
    "    df.loc[i+1] = ['overall (Macro)', recall_ma, precision_ma, f1_score_ma]\n",
    "    # df.loc[i+2] = ['overall (Micro)', recall_overall, precision_overall, f1_score_overall]\n",
    "    df['recall'] = df['recall'].apply(lambda x: round(x*100, 2))\n",
    "    df['precision'] = df['precision'].apply(lambda x: round(x*100, 2))\n",
    "    df['f1-score'] =  df['f1-score'].apply(lambda x: round(x*100, 2))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTCRF(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (hidden): Linear(in_features=768, out_features=22, bias=True)\n",
       "  (crf): ConditionalRandomField()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './model/bert-ner_best_test_v2.ckpt'\n",
    "bert_config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "embedding_dim = bert_config.pooler_fc_size\n",
    "num_class = len(ner2int)\n",
    "# vocab_size = bert_config.vocab_size\n",
    "# model = BiLSTMCRF(vocab_size, embedding_dim, num_class+1)\n",
    "# word_emb = BertModel.from_pretrained(PRETRAINED_MODEL_NAME).embeddings.word_embeddings.weight.data.to(device)\n",
    "# model.embedding.weight.data.copy_(word_emb)\n",
    "# model = BERTransformerCRF(bert_config, embedding_dim, num_class+1, torch.device('cuda:%d'%(gpu) if torch.cuda.is_available() else 'cpu'))\n",
    "model = BERTCRF(bert_config, embedding_dim, num_class+1)\n",
    "model.load_state_dict(torch.load(model_path), strict=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data/cluener_test_v2.csv')\n",
    "test_json = {}\n",
    "for i in range(len(test)):\n",
    "    test_json[test.id.loc[i]] = test.content.loc[i].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:08<00:00,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 8s, sys: 18 s, total: 5min 26s\n",
      "Wall time: 8.22 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gg = 0\n",
    "submit = {}\n",
    "for batch in tqdm(valid_iterator):\n",
    "    ids = [str(int(x)) for x in batch.id.detach().cpu().numpy()]\n",
    "    pred = inference(batch, model, device, True)\n",
    "    for idx in range(len(pred)):\n",
    "        id_ = int(ids[idx])\n",
    "        submit[id_] = {}\n",
    "        submit[id_]['text'] = ''.join(test_json[id_])\n",
    "        predict_entity = ner_extract(pred[idx])\n",
    "        tmp = {}\n",
    "        for atr, indexes in predict_entity.items():\n",
    "            tmp[atr] = {}\n",
    "            for index in indexes:\n",
    "                ent = ''.join(test_json[id_][index[0]: index[1]+1])\n",
    "                if ent not in tmp[atr]:\n",
    "                    tmp[atr][ent] = []\n",
    "                tmp[atr][ent].append(index)\n",
    "        submit[id_]['label'] = tmp\n",
    "#         if id_ == 21:\n",
    "#             gg = 1\n",
    "#             break\n",
    "#     if gg == 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(input_ordered_dict):\n",
    "    return loads(dumps(input_ordered_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluener_predict = []\n",
    "for i in range(len(submit)):\n",
    "    tmp = {}\n",
    "    tmp['id'] = i\n",
    "    tmp['label'] = to_dict(collections.OrderedDict(sorted(submit[i]['label'].items())))\n",
    "    cluener_predict.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '美国导弹防御局称，两枚导弹都“发射正常”，然而“海基x波段雷达”并未如先前预计地一样正常工作。',\n",
       " 'label': {'government': {'美国导弹防御局': [[0, 6]]}}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 100, 'label': {'government': {'美国导弹防御局': [[0, 6]]}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluener_predict[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/cluener_predict.json', 'w', encoding='utf8') as output:\n",
    "    for i in range(len(cluener_predict)):\n",
    "        output.write(json.dumps(cluener_predict[i], ensure_ascii=False))\n",
    "        output.write('\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity level  Performance ( Evaluate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:07<00:00,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 33s, sys: 16.2 s, total: 4min 49s\n",
      "Wall time: 7.61 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TP = {}\n",
    "FP = {}\n",
    "FN = {}\n",
    "for batch in tqdm(valid_iterator):\n",
    "    ids = [str(int(x)) for x in batch.id.detach().cpu().numpy()]\n",
    "    pred = inference(batch, model, device)[1]\n",
    "    y = []\n",
    "    for i in range(len(batch.ner)):\n",
    "        tmp = batch.ner[i].detach().cpu().numpy()\n",
    "        y.append(list(tmp[tmp != len(ner2int)]))\n",
    "    for idx in range(len(y)):\n",
    "        predict_entity = ner_extract(pred[idx])\n",
    "        ground_truth = ner_extract(y[idx])\n",
    "        for e, v in predict_entity.items():\n",
    "            if e not in ground_truth:\n",
    "                if e not in FP:\n",
    "                    FP[e] = 0\n",
    "                FP[e] += len(v)\n",
    "            else:\n",
    "                for l in v:\n",
    "                    if l in ground_truth[e]:\n",
    "                        if e not in TP:\n",
    "                            TP[e] = 0\n",
    "                        TP[e] += 1\n",
    "                    else:\n",
    "                        if e not in FP:\n",
    "                            FP[e] = 0\n",
    "                        FP[e] += 1\n",
    "        for e, v in ground_truth.items():\n",
    "            if e not in predict_entity:\n",
    "                if e not in FN:\n",
    "                    FN[e] = 0\n",
    "                FN[e] += len(v)\n",
    "            else:\n",
    "                for l in v:\n",
    "                    if l not in predict_entity[e]:\n",
    "                        if e not in FN:\n",
    "                            FN[e] = 0\n",
    "                        FN[e] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = performance_entity(TP, FP, FN)\n",
    "performance['bert-base'] = [88.75, 79.43, 78.89, 81.42, 60.89, 86.42, 87.03, 65.10, 73.68, 85.82, 78.82]\n",
    "performance['roberta-large'] = [89.09, 82.34, 79.62, 83.02, 62.63, 86.80, 88.17, 70.49, 74.60, 87.46, 80.42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>bert-base</th>\n",
       "      <th>roberta-large</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name</td>\n",
       "      <td>88.60</td>\n",
       "      <td>83.74</td>\n",
       "      <td>86.10</td>\n",
       "      <td>88.75</td>\n",
       "      <td>89.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>organization</td>\n",
       "      <td>79.29</td>\n",
       "      <td>78.44</td>\n",
       "      <td>78.86</td>\n",
       "      <td>79.43</td>\n",
       "      <td>82.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>position</td>\n",
       "      <td>79.91</td>\n",
       "      <td>78.46</td>\n",
       "      <td>79.18</td>\n",
       "      <td>78.89</td>\n",
       "      <td>79.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>company</td>\n",
       "      <td>81.48</td>\n",
       "      <td>78.77</td>\n",
       "      <td>80.10</td>\n",
       "      <td>81.42</td>\n",
       "      <td>83.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>address</td>\n",
       "      <td>62.47</td>\n",
       "      <td>66.95</td>\n",
       "      <td>64.63</td>\n",
       "      <td>60.89</td>\n",
       "      <td>62.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>game</td>\n",
       "      <td>87.46</td>\n",
       "      <td>74.57</td>\n",
       "      <td>80.50</td>\n",
       "      <td>86.42</td>\n",
       "      <td>86.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>government</td>\n",
       "      <td>86.23</td>\n",
       "      <td>79.48</td>\n",
       "      <td>82.72</td>\n",
       "      <td>87.03</td>\n",
       "      <td>88.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>scene</td>\n",
       "      <td>79.43</td>\n",
       "      <td>68.60</td>\n",
       "      <td>73.61</td>\n",
       "      <td>65.10</td>\n",
       "      <td>70.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>book</td>\n",
       "      <td>77.27</td>\n",
       "      <td>79.33</td>\n",
       "      <td>78.29</td>\n",
       "      <td>73.68</td>\n",
       "      <td>74.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>movie</td>\n",
       "      <td>78.15</td>\n",
       "      <td>87.41</td>\n",
       "      <td>82.52</td>\n",
       "      <td>85.82</td>\n",
       "      <td>87.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>overall (Macro)</td>\n",
       "      <td>80.03</td>\n",
       "      <td>77.57</td>\n",
       "      <td>78.78</td>\n",
       "      <td>78.82</td>\n",
       "      <td>80.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             entity  recall  precision  f1-score  bert-base  roberta-large\n",
       "0              name   88.60      83.74     86.10      88.75          89.09\n",
       "1      organization   79.29      78.44     78.86      79.43          82.34\n",
       "2          position   79.91      78.46     79.18      78.89          79.62\n",
       "3           company   81.48      78.77     80.10      81.42          83.02\n",
       "4           address   62.47      66.95     64.63      60.89          62.63\n",
       "5              game   87.46      74.57     80.50      86.42          86.80\n",
       "6        government   86.23      79.48     82.72      87.03          88.17\n",
       "7             scene   79.43      68.60     73.61      65.10          70.49\n",
       "8              book   77.27      79.33     78.29      73.68          74.60\n",
       "9             movie   78.15      87.41     82.52      85.82          87.46\n",
       "10  overall (Macro)   80.03      77.57     78.78      78.82          80.42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
